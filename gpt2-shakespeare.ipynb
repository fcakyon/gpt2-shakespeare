{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2cd4bb2-5af8-4466-af88-1bf1bb572033",
   "metadata": {
    "id": "a2cd4bb2-5af8-4466-af88-1bf1bb572033"
   },
   "source": [
    "# GPT2 Language Model Fine-tuning with Texts from Shakespeare\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fcakyon/gpt2-shakespeare/blob/main/gpt2-shakespeare.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd0b1e-d60f-4670-a3fd-287fe1adb91d",
   "metadata": {
    "id": "64dd0b1e-d60f-4670-a3fd-287fe1adb91d"
   },
   "source": [
    "## 0. Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f20278d6-aca6-49ea-99f0-95f470e870d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f20278d6-aca6-49ea-99f0-95f470e870d0",
    "outputId": "bc0e832e-3162-49ec-d138-42694a4a1f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in /usr/local/lib/python3.7/dist-packages (4.7.0)\n",
      "Requirement already up-to-date: datasets in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
      "Requirement already up-to-date: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
      "Requirement already up-to-date: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers datasets torch sentencepiece pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77eefb-59f5-4beb-8f8d-8933ebb4bf12",
   "metadata": {
    "id": "9d77eefb-59f5-4beb-8f8d-8933ebb4bf12"
   },
   "source": [
    "## 1. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3eb70-96ad-43fe-9983-3d2d59e8b942",
   "metadata": {
    "id": "d1d3eb70-96ad-43fe-9983-3d2d59e8b942"
   },
   "source": [
    "- Import required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "accfc14f-4070-439e-a7ff-28dee7f97557",
   "metadata": {
    "id": "accfc14f-4070-439e-a7ff-28dee7f97557"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, HfArgumentParser, TrainingArguments, Trainer, default_data_collator\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ccb2a-51d1-4254-8ff1-56856964a932",
   "metadata": {
    "id": "689ccb2a-51d1-4254-8ff1-56856964a932"
   },
   "source": [
    "- Initialize a GPT2 model with a language modelling head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "439153af-f26a-48a4-91dd-c76a266b68bc",
   "metadata": {
    "id": "439153af-f26a-48a4-91dd-c76a266b68bc"
   },
   "outputs": [],
   "source": [
    " model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d793669-6b47-46c4-b9be-cc263bfded7d",
   "metadata": {
    "id": "6d793669-6b47-46c4-b9be-cc263bfded7d"
   },
   "source": [
    "- Initialize GPT2 tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5c44cdd-5709-4246-a466-4d859ecc870b",
   "metadata": {
    "id": "c5c44cdd-5709-4246-a466-4d859ecc870b"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa6f0053-779d-42cb-ac7a-bbc383522770",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa6f0053-779d-42cb-ac7a-bbc383522770",
    "outputId": "7cabbdcb-ef3a-4a21-9011-bbc60fb1dbd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891535cb-d100-4fb3-a741-e3249cf6099f",
   "metadata": {
    "id": "891535cb-d100-4fb3-a741-e3249cf6099f"
   },
   "source": [
    "## 2. Initialize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51084c-8a17-4007-8ec0-5ffeef7dd53f",
   "metadata": {
    "id": "2c51084c-8a17-4007-8ec0-5ffeef7dd53f"
   },
   "source": [
    "- Download Shakespeare dataset from [Huggingface datasets hub](https://github.com/huggingface/datasets/blob/master/datasets/tiny_shakespeare/tiny_shakespeare.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8599c5a9-e5e6-4e4e-ac2d-14594bb6e260",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8599c5a9-e5e6-4e4e-ac2d-14594bb6e260",
    "outputId": "943c2fca-b986-4483-d186-3daf69e4c2e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tiny_shakespeare (lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    }
   ],
   "source": [
    "# download and load the dataset from the hub\n",
    "dataset_name = \"tiny_shakespeare\"\n",
    "cache_dir = \"lm_dataset/\"\n",
    "datasets = load_dataset(dataset_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54795fe-e850-4ee4-b9da-5997d5e9b934",
   "metadata": {
    "id": "b54795fe-e850-4ee4-b9da-5997d5e9b934"
   },
   "source": [
    "- Tokenize all the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a2e234f-03c7-43c3-96e9-0b68623fe4e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a2e234f-03c7-43c3-96e9-0b68623fe4e2",
    "outputId": "27780df0-02ad-4b31-935f-78d943a888ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-86ddf4b956c3d870.arrow\n",
      "Loading cached processed dataset at lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-8f8c3b07ee3d7fd9.arrow\n",
      "Loading cached processed dataset at lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-b018bddd97924ae8.arrow\n"
     ]
    }
   ],
   "source": [
    "column_names = datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # truncate dataset with max accepted size of the model\n",
    "    output = tokenizer(examples[text_column_name])\n",
    "    return output\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5785f33-fcc2-4e40-a09d-6069f66b8cd6",
   "metadata": {
    "id": "b5785f33-fcc2-4e40-a09d-6069f66b8cd6"
   },
   "source": [
    "- Split whole dataset into smaller sets of blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b591dd34-f780-464a-8c46-a613ecf3a23d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b591dd34-f780-464a-8c46-a613ecf3a23d",
    "outputId": "d43c265e-806e-4efe-f501-58ce397ad8ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-6991fd94a3bf1e9f.arrow\n",
      "Loading cached processed dataset at lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-b046afe72f1f5889.arrow\n",
      "Loading cached processed dataset at lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-ddf2d61014c59483.arrow\n"
     ]
    }
   ],
   "source": [
    "# get block size (max input length of the model)\n",
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    block_size = 1024\n",
    "    \n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# split total dataset into smaller sets of length block_size\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "540d0c3b-116e-4964-978e-4baaac4ffbea",
   "metadata": {
    "id": "540d0c3b-116e-4964-978e-4baaac4ffbea"
   },
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb1ba9-bfa1-4bd5-8358-84a37cc5bd1a",
   "metadata": {
    "id": "a2cb1ba9-bfa1-4bd5-8358-84a37cc5bd1a"
   },
   "source": [
    "## 3. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7da09e8f-0e8c-424c-bf2f-f30f741632c3",
   "metadata": {
    "id": "7da09e8f-0e8c-424c-bf2f-f30f741632c3"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = \"output/\", per_device_train_batch_size=1, num_train_epochs=30, save_total_limit=1)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f166b8-52ce-4e06-954e-ff38c369ac37",
   "metadata": {
    "id": "c1f166b8-52ce-4e06-954e-ff38c369ac37"
   },
   "source": [
    "# 4. Perform Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bab45c4e-c933-4682-a22f-a383fb029c2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "bab45c4e-c933-4682-a22f-a383fb029c2a",
    "outputId": "f38617d8-2ef2-42b0-c1ae-0a5ecf6c3f2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8820' max='8820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8820/8820 1:03:09, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.751200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.765900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.540600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform training\n",
    "train_result = trainer.train()\n",
    "\n",
    "# saves the tokenizer\n",
    "trainer.save_model()\n",
    "\n",
    "# save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "# save training state\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73d73f-f3a8-4643-b87f-2858581d0d7f",
   "metadata": {
    "id": "ca73d73f-f3a8-4643-b87f-2858581d0d7f"
   },
   "source": [
    "# 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "121cb633-e66d-43c3-b9a4-7efc7d8f9305",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "121cb633-e66d-43c3-b9a4-7efc7d8f9305",
    "outputId": "a9950789-9b81-4d0a-f89d-c1f46f8e5e45"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform evaluation over validation data\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "# calculate perplexity\n",
    "try:\n",
    "    perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n",
    "    \n",
    "# save perplexity\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57939bd5-e279-4a65-b350-061b453ea296",
   "metadata": {
    "id": "57939bd5-e279-4a65-b350-061b453ea296"
   },
   "source": [
    "# 6. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b21c021e-f759-446a-8394-1241bdefc4fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b21c021e-f759-446a-8394-1241bdefc4fe",
    "outputId": "ce2a7b33-820f-42b6-802a-ba69b8d79b53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "One does not simply walk into a room,\n",
      "And with his or her eyes fix fixed on nothing;\n",
      "Consenting nothing but his or her images and thoughts;\n",
      "And, with himself or nobleness, nothing but himself or\n",
      "his or her images: so he, nothing but himself,\n",
      "His image flatters himself: just as two men\n",
      "Are one flat, one flat, one good, one good for another;\n",
      "For if he did not wish to be but one image,\n"
     ]
    }
   ],
   "source": [
    "# fix seed\n",
    "import torch\n",
    "torch.manual_seed(2)\n",
    "\n",
    "# tokenize start of a sentence\n",
    "ids = tokenizer.encode('One does not simply walk into',\n",
    "                      return_tensors='pt').cuda()\n",
    "\n",
    "# generate samples by top-p sampling\n",
    "sample_output = model.generate(\n",
    "    ids, \n",
    "    do_sample=True, \n",
    "    max_length=100, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "# print generated texts\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0df83-4cfe-4e51-b9f6-3b8e2b89374d",
   "metadata": {
    "id": "a0f0df83-4cfe-4e51-b9f6-3b8e2b89374d"
   },
   "source": [
    "# 7. Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66dd562-ccba-4bd6-8609-54b07ed59b62",
   "metadata": {
    "id": "d66dd562-ccba-4bd6-8609-54b07ed59b62"
   },
   "outputs": [],
   "source": [
    "kwargs = {\"finetuned_from\": \"gpt2\", \"tasks\": \"text-generation\"}\n",
    "kwargs[\"dataset_tags\"] = \"tiny_shakespeare\"\n",
    "kwargs[\"dataset_args\"] = \"default\"\n",
    "kwargs[\"dataset\"] = \"tiny_shakespeare default\"\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "gpt2-shakespeare.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3.8 (turkish-qa-qg)",
   "language": "python",
   "name": "turkish-qa-qg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
