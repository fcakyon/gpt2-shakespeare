{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2cd4bb2-5af8-4466-af88-1bf1bb572033",
   "metadata": {},
   "source": [
    "# GPT2 Language Model Fine-tuning with Texts from Shakespeare\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fcakyon/gpt2-shakespeare/blob/main/gpt2-shakespeare.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd0b1e-d60f-4670-a3fd-287fe1adb91d",
   "metadata": {},
   "source": [
    "## 0. Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20278d6-aca6-49ea-99f0-95f470e870d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets torch sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77eefb-59f5-4beb-8f8d-8933ebb4bf12",
   "metadata": {},
   "source": [
    "## 1. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3eb70-96ad-43fe-9983-3d2d59e8b942",
   "metadata": {},
   "source": [
    "- Import required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accfc14f-4070-439e-a7ff-28dee7f97557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, HfArgumentParser, TrainingArguments, Trainer, default_data_collator\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ccb2a-51d1-4254-8ff1-56856964a932",
   "metadata": {},
   "source": [
    "- Initialize a GPT2 model with a language modelling head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439153af-f26a-48a4-91dd-c76a266b68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d793669-6b47-46c4-b9be-cc263bfded7d",
   "metadata": {},
   "source": [
    "- Initialize GPT2 tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c44cdd-5709-4246-a466-4d859ecc870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6f0053-779d-42cb-ac7a-bbc383522770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891535cb-d100-4fb3-a741-e3249cf6099f",
   "metadata": {},
   "source": [
    "## 2. Initialize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51084c-8a17-4007-8ec0-5ffeef7dd53f",
   "metadata": {},
   "source": [
    "- Download Shakespeare dataset from [Huggingface datasets hub](https://github.com/huggingface/datasets/blob/master/datasets/tiny_shakespeare/tiny_shakespeare.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8599c5a9-e5e6-4e4e-ac2d-14594bb6e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tiny_shakespeare/default (download: 1.06 MiB, generated: 1.06 MiB, post-processed: Unknown size, total: 2.13 MiB) to lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b73b27f5a224d338ee13870d4b14fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435071.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tiny_shakespeare downloaded and prepared to lm_dataset/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# download and load the dataset from the hub\n",
    "dataset_name = \"tiny_shakespeare\"\n",
    "cache_dir = \"lm_dataset/\"\n",
    "datasets = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "if \"validation\" not in datasets.keys():\n",
    "    datasets[\"validation\"] = load_dataset(\n",
    "        dataset_name,\n",
    "        split=f\"train[:10%]\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    datasets[\"train\"] = load_dataset(\n",
    "        dataset_name,\n",
    "        split=f\"train[10%:]\",\n",
    "        cache_dir=cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54795fe-e850-4ee4-b9da-5997d5e9b934",
   "metadata": {},
   "source": [
    "- Tokenize all the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a2e234f-03c7-43c3-96e9-0b68623fe4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbd87fb348c4ebb8080cfb924fc0e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on dataset', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c26dd577d9a45939b29294f3138f58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on dataset', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533358226f2643439fcc39dce2b2c505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on dataset', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # truncate dataset with max accepted size of the model\n",
    "    output = tokenizer(examples[text_column_name])\n",
    "    return output\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5785f33-fcc2-4e40-a09d-6069f66b8cd6",
   "metadata": {},
   "source": [
    "- Split whole dataset into smaller sets of blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b591dd34-f780-464a-8c46-a613ecf3a23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebfcd657a954c5c8103c28f80d4c19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 1024', max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea92557359a451eb5a1eed6b8c6a8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 1024', max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd93d71a40b4af38b3594879d7ec18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 1024', max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get block size (max input length of the model)\n",
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    block_size = 1024\n",
    "    \n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# split total dataset into smaller sets of length block_size\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540d0c3b-116e-4964-978e-4baaac4ffbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb1ba9-bfa1-4bd5-8358-84a37cc5bd1a",
   "metadata": {},
   "source": [
    "## 3. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da09e8f-0e8c-424c-bf2f-f30f741632c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = \"output/\", per_device_train_batch_size=1)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f166b8-52ce-4e06-954e-ff38c369ac37",
   "metadata": {},
   "source": [
    "# 4. Perform Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab45c4e-c933-4682-a22f-a383fb029c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform training\n",
    "train_result = trainer.train()\n",
    "\n",
    "# saves the tokenizer\n",
    "trainer.save_model()\n",
    "\n",
    "# save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "# save training state\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73d73f-f3a8-4643-b87f-2858581d0d7f",
   "metadata": {},
   "source": [
    "# 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f7a60-c7d3-4d07-84bf-08213164a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform evaluation over validation data\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "# calculate perplexity\n",
    "try:\n",
    "    perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n",
    "    \n",
    "# save perplexity\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0df83-4cfe-4e51-b9f6-3b8e2b89374d",
   "metadata": {},
   "source": [
    "# 6. Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66dd562-ccba-4bd6-8609-54b07ed59b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"finetuned_from\": \"gpt2\", \"tasks\": \"text-generation\"}\n",
    "kwargs[\"dataset_tags\"] = \"tiny_shakespeare\"\n",
    "kwargs[\"dataset_args\"] = \"default\"\n",
    "kwargs[\"dataset\"] = \"tiny_shakespeare default\"\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 (turkish-qa-qg)",
   "language": "python",
   "name": "turkish-qa-qg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
